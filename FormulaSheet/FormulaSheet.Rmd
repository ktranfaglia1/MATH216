---
title: "All in one R functions"
author: "Kyle Tranfaglia"
date: "2023-10-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE,message=FALSE)

library(dplyr)
library(ggplot2)
library(ggpubr)
library(flextable)
library(tidyr)
library(knitr)
library(tidyverse)
library(extraDistr)
library(nortest)
library(car)
library(BSDA)
```

# Importing Data file

```{r}
data0 <- read.csv("US_STATES.csv")
```

# Data Analytics

```{r}
VectorTest <- 1:10

VectorTest

data0[1,] # Display first row
data0[,2] # Display second column
data0[1:5,2:5] # Display first 5 rows and columns 2-5
data0[data0$Name=="Maryland",] # Find the information about Maryland
subset(data0, Name=="Maryland") # Find the information about Maryland
data0$Name # Find the information about all the names of the states
data0[data0$Region=="South",]$Name # Only display the names for all the states in the region “South”
subset(data0, Region=="South")$Name
select(data0[data0$Region=="South",],Name,Area,Population) # Only display the names, Area, and Population for all the states in the region “South”
select(data0[data0$Region=="South" & data0$Population>3000,],Name,Area,Population) # Only display the names, Areas, and Population for all the states in the region “South” AND have populations >3000
NEstates <- select(data0[data0$Region=="Northeast" & data0$Population>5000,],Name,Area,Population)
kable(NEstates) # Only display the names, Areas, and Population for all the states in the region “Northeast” AND have populations >5000 and display that object using kable function

DesStat <- function(datacol, title){ # Function for descriptive statistic table
  datasummary <- as.data.frame(list(N=length(datacol),min=min(datacol),firstQ=quantile(datacol,0.25),median=median(datacol),thirdQ=quantile(datacol,0.75),max=max(datacol),mean=round(mean(datacol),3),StDev=round(sd(datacol),3),Var=round(var(datacol),3)))
  
  
  datasummary<-flextable(datasummary) 
  datasummary<-set_caption(datasummary, title)
  
  save_as_docx(datasummary, path=paste(title,".docx"))
  datasummary
}
DesStat(data0$LifeExp, "Descriptive Statistics - life expectancy - Kyle Tranfaglia") # Function call

# Code for Frequency Table
FrequencyTable <- data0 %>% group_by(data0$Region, data0$HS_Grad) %>%
  summarise(Count=n())
FrequencyTable <- flextable(FrequencyTable)
FrequencyTable <- set_caption(FrequencyTable, "Frequency Table of Height/Sex - Kyle Tranfaglia")
FrequencyTable

# Function for comparative histogram
HistWithGroups <- function(data,xcol,xname,group,gname,bwidth){
  ggplot(data, aes(x=xcol,fill=group))+
    geom_histogram(aes(y=..density..),color="black",binwidth=bwidth,alpha=0.6, position="identity")+
    scale_y_continuous(labels=scales::percent_format())+
    xlab(xname)+
    scale_fill_discrete(name=gname)
  
}
data1 <- read.csv("Pulse.csv") # Open file to read data ... store in data2
HistWithGroups(data1,data1$Height,"Height",data1$Sex,"Sex",1) # Function call

ggplot(data1, aes(x=data1$Height))+
  geom_histogram(binwidth=1,color="black", fill="lightblue") # Simple ggplot template

# Pre-plotting and getting tally table
data1$Sex[data1$Sex==1]<-'Male'
data1$Sex[data1$Sex==2]<-'Female'

data1$Activity[data1$Activity==1]<-'Slight'
data1$Activity[data1$Activity==2]<-'Moderate'
data1$Activity[data1$Activity==3]<-'A lot'

SexCount <- data1 %>% group_by(Sex) %>%
  summarise(Count=n())


flextable(SexCount)

SexProportion <- round(SexCount$Count/sum(SexCount$Count),2)

SexStat <- cbind(SexCount,SexProportion)

flextable(SexStat)

## ggplot pie chart
ggplot(SexStat,aes(x="",y=SexProportion,fill=Sex))+
  geom_bar(stat="identity")+
  coord_polar("y",start=0)+
  theme_void()+
  geom_text(aes(label=paste("Count=",Count,"  Prop=", scales::percent(SexProportion))),position=position_stack(vjust=0.5))+
  scale_fill_manual(values=c("lightblue","orange"))

# Cross-tab
data1$Smokes[data1$Smokes==1]<-'Yes'
data1$Smokes[data1$Smokes==2]<-'No'

CrossCount3 <- data1 %>% group_by(Sex, Smokes) %>%
  tally()

CrossCount4 <- data1 %>% group_by(Sex, Smokes) %>%
  tally() %>%
  spread(Sex,n)

CrossCount4
```

# Check Normality

```{r}
hist(data0$LifeExp)
```

```{r}
library(car)

qqPlot(data0$LifeExp)
```

# Compute Mean, Variance, Standard Deviation, Quantile from Data Set

```{r}

lfexp <- data0$LifeExp

mean(lfexp)

var(lfexp)

sd(lfexp)

summary(lfexp)

length(lfexp)

min(lfexp)

max(lfexp)

median(lfexp)

quantile(lfexp)

quantile(lfexp, 0.25)

quantile(lfexp, 0.75)
```

in R,

p is usually for area under the curve and to the left of the cutting point - P[x\<=q]

q is the cutting point that p of the data will be to the left of that point

d is for discrete

binom for Binomial

pois for Poisson

norm for normal

# Binomial Random Variable

Assume the following random variable follows a binomial distribution with n=20, p=0.6

```{r}
# Function to plot Binomial random variables
Binom_plotter<-function(n,p,lb,ub){
x<-seq(0,n)
hx<-dbinom(x,n,p)


i<-x>= lb & x<= ub

hxi<-dbinom(x[i],n,p)


plot(x,hx,type='h', xlim=c(0,n),ylim=c(0,max(hx)))



if (lb>=0&lb<=n){
  abline(v=lb,lty=3,col='blue')
}

if (ub>=0&ub<=n){
  abline(v=ub,lty=3,col='blue')
}
par(new=T)
plot(x[i],hxi,type='h', col="red", xlim=c(0,n),ylim=c(0,max(hx)),xlab="",ylab="" )

par(new=F)


area <- pbinom(ub, n, p) - pbinom(lb, n, p) + dbinom(lb,n,p)
result <- paste("P(",lb,"\u2264 X \u2264",ub,") =",
   signif(area, digits=3), "\n",
   "n=", n, " p=", p)
mtext(result,3)

}
```

Example 1: Find P[x\<=13]

```{r}

pbinom(q=13, size=20, prob=0.6)

Binom_plotter(20,0.6,0,13)
```

Example 2: Find P[x\<13]

```{r}

pbinom(q=12, size=20, prob=0.6)
```

Example 3: Find P[x=13]

```{r}

dbinom(x=13, size=20, prob=0.6)
Binom_plotter(20,0.6,13,13)
```

Example 4: Find P[4\<x\<=13]

```{r}

pbinom(q=13, size=20, prob=0.6) - pbinom(q=4, size=20, prob=0.6)
Binom_plotter(20,0.6,5,13)
```

Example 5: Find P[4\<=x\<13]

```{r}

pbinom(q=12, size=20, prob=0.6) - pbinom(q=3, size=20, prob=0.6)
```

Example 6: Find P[4\<=x\<=13]

```{r}

pbinom(q=13, size=20, prob=0.6) - pbinom(q=3, size=20, prob=0.6)
```

Example 7: Find P[4\<x\<13]

```{r}

pbinom(q=12, size=20, prob=0.6) - pbinom(q=4, size=20, prob=0.6)
```

Example 8: Find P[x\>=13]

```{r}

pbinom(q=20, size=20, prob=0.6) - pbinom(q=12, size=20, prob=0.6)
```

Example 9: Find P[x\>13]

```{r}

pbinom(q=20, size=20, prob=0.6) - pbinom(q=13, size=20, prob=0.6)
Binom_plotter(20,0.6,14,20)
```

# Poisson Random Variable

rpois(n,lambda)-Which generates a random sample from the Poisson distribution

dpois(x,lambda)-generates $P(X=x)$ for the Poisson distribution

ppois(q,lambda)- calculates $P(X\leq x)$ for a Poisson distribution

qpois(p,lambda)- This gives the x value such that $P(X \leq x)=p$ for a Poisson distribution

suppose $\lambda=2.2$ Find $P(X>3)$ and $P(2.2< X < 5.1)$

```{r}

1 - ppois(3, 2.2)

ppois(5.1, 2.2) - ppois(2.2,2.2)
```

# Normal Random Variable

Given p, find q

Given q, find p

Different P[....]

Given that q, the cutting is 20 with a mean of 27 and a sd of 3, find p, as in find $P(X < 20)$

```{r}

pnorm(q = 20, mean = 27, sd = 3)
```

Given that p, the area under the curve is 0.90, being the 90th percentile, and there is a mean of 550 and sd of 100, find q, as in the corresponding cutting point x, for p(x) = 0.90

```{r}

qnorm(p = .90, mean = 550, sd = 100)
```

Find z0 such that 95% of the standard normal z values lie between -z0 and z0. So, find z0 such that $P(-z0 <= z <= z0) = 0.95$

```{r}

qnorm(p = 0.975, mean = 0, sd = 1)
```

# Uniform Distribution

```{r}
# define the x-axis
x_uniform<-seq(1,30, length=30)

# calculate the uniform probabilities
y_uniform<- ddunif(x_uniform,min=1,max=30)

# plot the distribution
plot(x_uniform,y_uniform, type='h')


```

To calculate a probability of $P(5< X \leq 7)$ we would o the following:

```{r}

round(pdunif(7,1,30)-pdunif(5,1,30),5) 
```

However, in the case of $P(5\leq X\leq 7)$, we should do some adjustments.

```{r}

round(pdunif(7,1,30)-pdunif(4,1,30),5)
```

Now calculate $P(10 < X \leq 18.1)$ and $P(10 \leq X \leq 18.1)$

```{r}

round(pdunif(18.1,1,30)-pdunif(10,1,30),10) 
round(pdunif(18.1,1,30)-pdunif(9,1,30),10) 
```

To get a list of random data in R we can do the following:

```{r}

unifrom_sample<-rdunif(100,1,30) 
```

We can then plot the histogram using the hist function:

```{r}
# basic plot 
hist(unifrom_sample) 
# correct the bining 
hist(unifrom_sample, breaks=c(1,6,11,16,21,26,30),xlim=c(1,30))
```

We can also overlay the distribution over the histogram:

```{r}

a<-1
b<-30
x_uniform<-seq(a,b, length=b)

y_uniform<- dunif(x_uniform,min=a,max=b)

hist(unifrom_sample, breaks=c(1,6,11,16,21,26,30),xlim=c(1,30))

for (i in 1:30){
  segments(i,0,i,1/(b-a), lwd=2)
}   
```

# Confidence Interval

```{r}

CI_data <- function(data, PopSDknown, PopSDvalue, ConfCoef, digit ){
  
  # data should be a column of numeric data
  # PopSDknown should be "Yes" or "No", whether the Population standard deviation is known or not.
  # PopSDvalue should be the value of the pop sd value. If pop sd is NOT known, just input 0.
  # ConfCoef is the confidence coefficient, should be a real number between 0 and 1.
  # digit is the number of digits you want to present in the CI
  
  x_bar <- mean(data)
  n <- length(data)
  
  confLv <- ConfCoef*100
  
  if (PopSDknown == "Yes"){
        crval <- qnorm((1+ConfCoef)/2)
        SD <- PopSDvalue
        output1 <- paste("The population standard deviation is", bquote(.(PopSDvalue)))
        
  }
  else if (PopSDknown == "No"){
        crval = qt((1+ConfCoef)/2,n-1)
        SD <- sd(data)
        output1 <- paste("The population standard deviation is unknown.")
  }
  
  ci_ub <- round(x_bar + crval*SD/sqrt(n),digit)
  ci_lb <- round(x_bar - crval*SD/sqrt(n),digit)
  

  
  
  cat(output1,"\nThe sample size is", bquote(.(n)), ". The sample mean is", bquote(.(x_bar)), ". The sample standard deviation is", bquote(.(sd(data))),"\nThe", bquote(.(confLv)), "% Confidence Interval for the population mean is (", bquote(.(ci_lb)), bquote(.(ci_ub)), ").\n" )
  
}

CI_stat <- function(x_bar,n, PopSDknown, SD, ConfCoef, digit ){
  
  # x_bar is the sample mean
  # n is the sample size
  # PopSDknown should be "Yes" or "No", whether the Population standard deviation is known or not
  # SD is the pop sd or sample sd, depends on the input of PopSDknown
  # ConfCoef is the confidence coefficient, should be a real number between 0 and 1.
  # digit is the number of digits you want to present in the CI
  

  confLv <- ConfCoef*100
  
  if (PopSDknown == "Yes"){
        crval <- qnorm((1+ConfCoef)/2)
        output1 <- paste("The population standard deviation is", bquote(.(SD)))
        
  }
  else if (PopSDknown == "No"){
        crval = qt((1+ConfCoef)/2,n-1)
        output1 <- paste("The population standard deviation is unknown.")
  }
  
  ci_ub <- round(x_bar + crval*SD/sqrt(n),digit)
  ci_lb <- round(x_bar - crval*SD/sqrt(n),digit)

  
  cat(output1,"\nThe sample size is", bquote(.(n)), ". The sample mean is", bquote(.(x_bar)), ". The sample standard deviation is", bquote(.(SD)),"\nThe", bquote(.(confLv)), "% Confidence Interval for the population mean is (", bquote(.(ci_lb)), bquote(.(ci_ub)), ").\n" )
  
}
```

```{r}
x <- rnorm(100,5,3)
CI_data(x,"No",0,0.95,4)
CI_data(x,"Yes",3,0.95,4)

CI_stat(4.711773,100,"Yes",3,0.95,4)

# Insert Image Syntax: ![text about the image](image file name)
```

# Table Ploting

```{r}

Sample_Size <- c(10,40,90, 160, 250)
confidence_Proportion <- c(.94, .97,.96, .97, .94)
avg_Width <- c(1.33524,.63044,.41996,.31374,.24858)
CI_table_90_normal <- data.frame(Sample_Size , confidence_Proportion, avg_Width)

colnames(CI_table_90_normal)<-c("Sample Size", "Prop of CIs Covering True Mean", "Ave Width of CIs")

library(knitr)
kable(CI_table_90_normal,padding=6,align="ccc",caption="95% CIs from Normal Pop - Different Sample Sizes")


Parent_distribution <- c("Normal","Uniform","Right Skew", "Left Skew", "Bimodal", "Cauchy")
propofCI <- c(.88, .78, .78, .87, .79, .65)
numonLeft <- c(9, 9, 14, 6, 7, 30)
numonRight <- c(3, 13, 8, 7, 14, 5)
CI_table2 <- data.frame(Parent_distribution , propofCI, numonLeft, numonRight)

colnames(CI_table2)<-c("Parent Distribution", "Prop of CIs Covering True Mean", "Num of left CI not cover true mean", "Num of right CI not cover true mean")

library(knitr)
kable(CI_table2,padding=6,align="ccc",caption="80% CIs from Different Pop - Sample Size 500")
```

# Hypothesis Testing

```{r}

mydata<-read.csv("firstgrade.csv")

x<-mydata["Height"]

y<-mydata["Weight"]

# Since we are directly given data, we should use 1-sample t test (R code is t.test())

# x = data, mu = mu0 in the hypothesis
# Alternative is either "two.sided", "greater", or "less" ... "two.sided" if HA: mu != mu0
# conf.level = l - alpha

t.test(x=x, mu=42, alternative = "greater", conf.level = 1-0.05)

# To test a data set to determine whether or not it is reasonable to assume it is a random sample from a normal distribution we will use the Anderson-Darling Test
# Anderson-Darling Test

x_Z25<-rnorm(25,0,1)

ad.test(x_Z25) # Anderson Darling Test

qqPlot(x_Z25)

# t-test with summary statistics

tsum.test(mean.x=6.45,s.x=4,n.x=45,mu=6,alternative = 'greater',conf.level = .95)

# If sigma is known, and either you have large n, or small n but normal population, you can use z-test

# z.test(x=data, sigma.x=sigma, mu=mu0, alternative = "xxx", conf.level = 0.xx)

# zsum.test(mean.x=xbar, s.x=sigma, n.x=n, mu=mu0, alternative = "xxx", conf.level = 0.xx)

# Wilcoxon test
# wilcox.test(x=data, mu=mu0, alternative = "xxx", conf.level = 0.xx)

# Sign Test
# SIGN.test(x=data, md=md0, alternative = "xxx", conf.level = 0.xx)

# Shapiro Test
# shapiro.test(x=data)

# Two Sample T Test
# t.test(x=data1, y =data2, mu=md0, alternative = "two.sided", conf.level = 0.xx)

# Two Sample Z Test
# z.test(x=data1, y =data2, sigma.x=sigmax, sigma.y=sigmay, mu=md0, alternative = "two.sided", conf.level = 0.xx)

# Test symmetry. If value is less than 0.05, it is symmetric
# abs(mean(data)) - abs(median(data)) / abs(mean(data))

```

# Chi-Squared

```{r}
# Perform Chi-Squared test on a data set
chisq.test(x=c(218,497,425,157), p=c(0.185,0.392,0.412,0.011))
# Perform Chi-Squared given the contigenency table
tab1 <- matrix(c(24,9,13,289,100,565), nrow=2, ncol=3, byrow=TRUE)
tab1
chisq.test(tab1, correct=TRUE)


```

# Linear Regression

```{r}
# Get data
data22 <- read.csv("FinalPrac22.csv")
x <- data22$X
y <- data22$Y
# Fit a linear regression model. y ~ x: This formula notation specifies the relationship between the response variable (y) and the predictor variable (x). In a linear regression model, you are trying to model the relationship between the response variable and one or more predictor variables
mod22<-lm(y~x)
# Get detailed information about the fitted mode
summary(mod22)
# Performs an analysis of variance on the fitted model, comparing the fit of the model with the null hypothesis to assess the overall significance of the model.
anova(mod22)
# Compute confidence intervals for the estimated coefficients of a fitted model
confint(mod22, level = 0.95)
# Calculates the correlation between two numeric vectors x and y. Correlation is a statistical measure that quantifies the strength and direction of a linear relationship between two variables. The result ranges from -1 to 1: 1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, 0 indicates no linear relationship.
cor(x,y)
# Obtain predicted values from a model. In the context of linear regression models, the predict() function predicts the response variable for new or existing data
predict(mod22, newData=data.frame(x=1.25), level = 0.94, interval = "confidence")
# Get prediction interval
predict(mod22, newData=data.frame(x=1.25), level = 0.94, interval = "predict")
```

# Quiz 2 Workspace

```{r}

Binom_plotter(3,0.2,0,4)
Binom_plotter(3,0.2,0,2)
pbinom(q=2, size=3, prob=0.2)

pnorm(q = 15, mean = 11, sd = 4) - pnorm(q = 10, mean = 11, sd = 4)
1 - pnorm(q = 20, mean = 11, sd = 4)
qnorm(p=0.90, mean = 11, sd = 4)
```

# Test 2 Practice Exam Workspace

```{r}
# Question 3 Practice Exam .. Gets probability distribution of x
x <- c(0, 1, 2, 3, 4)

dbinom(x, 4, 0.2)

pbinom(q=2, size=4, prob=0.2)

data2 <- read.csv("Exam2Prac7.csv")
hist(data2$Sample.Data)

normal_data <- rnorm(1000, mean = 0, sd = 1)
hist(normal_data)

mean(normal_data)
sd(normal_data)

mn <- mean(data2$SampleData)
SD <- sd(data2$SampleData)

mn
SD

# Exponential Distribution
# known cutting point q, to find area P[x<=q]
pexp(5, 1/6.25)
# Known area p[x<=q] to find cutting point q
qexp(0.275, 1/6.25)

1 - pnorm(82, 80, 1)

1 - pnorm(0.30, 0.27, 0.0444)
```

# Exam 2 Workspace

```{r}

data3 <- read.csv("Exam2.csv")
hist(data3$SampleData)

normal_data <- rnorm(1000, mean = 0, sd = 1)
hist(normal_data)

mean(normal_data)
sd(normal_data)

DataMean <- mean(data3$SampleData)
DataSD <- sd(data3$SampleData)

DataMean
DataSD

# Interval Bounds (92% Confidence)
qnorm(.96, DataMean, DataSD) 
qnorm(.04, DataMean, DataSD)

pnorm(q = 69, mean = 75, sd = 8) - pnorm(q = 60, mean = 75, sd = 8)

1 - pnorm(q = 90, mean = 75, sd = 8)

qnorm(.90, 75, 8)


x1 <- c(0, 1, 2, 3, 4, 5)

dbinom(x1, 5, 0.3)

pbinom(q=2, size=5, prob=0.3)

1 - pnorm(q = 23, mean = 20, sd = 2)

pnorm(q = 20, mean = 20, sd = 2)

pnorm(q = 23, mean = 20, sd = 2) - pnorm(q = 16, mean = 20, sd = 2)

1 - pnorm(q = 0.75, mean = 0.67, sd = 0.01487)

pnorm(q = 0.50, mean = 0.67, sd = 0.01487)

qnorm(.975, 120.1, 20)

```

# Quiz 3 Workspace

```{r}
Quiz3Data <- read.csv("SAT_Scores_100.csv")
Quizx <- Quiz3Data$SampleData

z.test(x=Quizx, sigma.x=116, mu=525, alternative = "greater", conf.level = 0.95)
sd(Quizx)

Quiz3Data1 <- read.csv("SAT_Scores_20.csv")
Quizy <- Quiz3Data1$SampleData

ad.test(Quizy)
qqPlot(Quizy)

t.test(x=Quizy, mu=530, alternative = "greater", conf.level = 0.97)
sd(Quizy)

tsum.test(mean.x=575.85,s.x=123.7903,n.x=20,mu=530,alternative = 'greater',conf.level = 0.97)

```

# Exam 3 Practice Workspace

```{r}
# Question 1
question1Data <- c(62, 75, 170, 47, 47, 81, 58, 2, 43, 52)

ad.test(question1Data)
qqPlot(question1Data)

(abs(mean(question1Data)) - abs(median(question1Data))) / abs(mean(question1Data))

#t.test(x=question1Data, mu=70, alternative = "less", conf.level = 0.90)
SIGN.test(x=question1Data, md=70, alternative = "less", conf.level = 0.90)

# Question 2
question2DataX <- c(210, 230, 219, 229, 223, 240, 315)
question2DataY <- c(126, 138, 127, 266, 174, 180, 147, 156, 148)

# ad.test(question2DataX) Does not work, sample size too small ... use shapiro test
shapiro.test(question2DataX)
qqPlot(question2DataX)

ad.test(question2DataY)
qqPlot(question2DataY)

wilcox.test(x=question2DataX, y=question2DataY, mu=0, alternative = "greater", conf.level = 0.95)

# Question 3
question3DataX <- c(57, 68, 77, 63, 72, 83)
question3DataY <- c(73, 69, 87, 69, 86, 84)

diff3 <- question3DataX - question3DataY

shapiro.test(diff3)
qqPlot(diff3)

t.test(diff3, mu=0, alternative = "less")
t.test(diff3, mu=0, alternative = "two.sided", conf.level = 0.97)

# Question 4
question4Data <- read.csv("Exam3Prac4.csv")
question4Datax <- question4Data$SampleData

ad.test(question4Datax)
qqPlot(question4Datax)

t.test(question4Datax, mu=8.5, alternative = "two.sided", conf.level = 0.93)

# Question 5
zsum.test(mean.x=548, sigma.x=116, n.x=80, mu=525, alternative = "greater")
zsum.test(mean.x=548, sigma.x=116, n.x=80, mu=525, alternative = "two.sided", conf.level=0.96)
```

# Exam 3 Workspace

```{r}
# Question 1
Data1 <- c(564, 498, 259, 303, 300, 307)

shapiro.test(Data1)
qqPlot(Data1)

t.test(Data1, mu=300, alternative = "greater", conf.level = 0.90)

# Question 2
Data2X <- c(104, 82, 102, 96, 129, 89, 114, 107, 89, 103)
Data2Y <- c(103, 103, 91, 113, 102, 103, 92, 90, 114, 113)

diff2 <- Data2X - Data2Y

ad.test(diff2)
qqPlot(diff2)

t.test(diff2, mu=0, alternative = "two.sided", conf.level = 0.95)

# Question 3
Data3X <- c(67, 68, 78, 75, 84)
Data3Y <- c(59, 63, 81, 74, 78)

diff3 <- Data3X - Data3Y

shapiro.test(diff3)
qqPlot(diff3)

t.test(diff3, mu=0, alternative = "less", conf.level = 0.95)
t.test(diff3, mu=0, alternative = "two.sided", conf.level = 0.97)

# Question 4
QData4 <- read.csv("Exam3Prob4.csv")
Data4 <- QData4$SampleData

ad.test(Data4)
qqPlot(Data4)

t.test(Data4, mu=8.5, alternative = "two.sided", conf.level = 0.93)

# Question 5
zsum.test(mean.x=4.98, sigma.x=1.62, n.x=258, mu=4.7, alternative = "greater")
zsum.test(mean.x=4.98, sigma.x=1.62, n.x=258, mu=4.7, alternative = "two.sided", conf.level=0.96)
```
